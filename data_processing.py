# -*- coding: utf-8 -*-
!pip install snownlp
!pip install gensim
!pip install --upgrade numpy
!pip install --upgrade gensim
!pip install stopwordsiso

import csv
import pandas as pd
from datetime import datetime

# Define the input and output file paths
input_file = '/content/D5.csv'        # Replace with the actual input file path
output_file = 'Dataset5_final.csv'    # Replace with the desired output file path

# Step 1: Read the input CSV file and add the "label" column
with open(input_file, 'r', newline='', encoding='utf-8') as infile:
    reader = csv.reader(infile)
    rows = list(reader)

if rows:
    # Add the "label" column in the header
    header = rows[0]
    header.append("label")

    # Locate the index for 'Status' and 'label'
    try:
        status_idx = header.index("Status")
        label_idx = header.index("label")
    except ValueError:
        print("CSVæ–‡ä»¶ä¸­å¿…é¡»åŒ…å«'status'å’Œ'label'åˆ—")
        exit()

    # Process the data rows
    for row in rows[1:]:
        # Ensure the row is long enough
        if len(row) < len(header):
            row.extend([''] * (len(header) - len(row)))  # Extend the row to match header length

        # Standardize and process the 'Status' and 'label' columns
        status_val = row[status_idx].strip()

        if status_val == "1":
            row[label_idx] = "1"
            row[status_idx] = "1"
        else:
            row[label_idx] = "0"
            row[status_idx] = "0"

# Step 2: Write the updated content to a new CSV file
with open(output_file, 'w', newline='', encoding='utf-8') as outfile:
    writer = csv.writer(outfile)
    writer.writerows(rows)

print(f"å¤„ç†å®Œæˆï¼Œæœ€ç»ˆæ–‡ä»¶å·²è¾“å‡ºä¸º {output_file}")

# Step 3: Process the saved CSV file using pandas
df = pd.read_csv(output_file, encoding='utf-8')

# 0. Data filtering
df = df[df['Type'] == 1]

# 1. Data processing: generating status column
df['label'] = df['label'].apply(lambda x: 1 if x == 1 else 0)

# 2. Time feature processing
df['timestamp'] = pd.to_datetime(df['CreateTime'], unit='s')
df['time_diff'] = df['timestamp'].diff().dt.total_seconds().fillna(0)

# 3. Text feature processing
df['message'] = df['StrContent'].fillna('').str.replace(r'<.*?>', '', regex=True)

# 4. User feature processing
user_mapping = {user: idx for idx, user in enumerate(df['Sender'].unique())}
df['user_id'] = df['Sender'].map(user_mapping)

# Keep only relevant columns
processed_df = df[['timestamp', 'user_id', 'message', 'time_diff', 'label']]
processed_df.reset_index(drop=True, inplace=True)

# Display the processed data
print(processed_df.head())

# Save the processed data to a new CSV file
processed_df.to_csv("processed_chat_data_5.csv", index=False, encoding="utf-8")
print("å¤„ç†åçš„æ•°æ®å·²ä¿å­˜ä¸º processed_chat_data_5.csv")

from transformers import DistilBertTokenizer, DistilBertModel
import torch

# æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# åŠ è½½ DistilBERT æ¨¡å‹å’Œ Tokenizerï¼Œå¹¶å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)

# å°†æ¶ˆæ¯è½¬åŒ–ä¸º DistilBERT åµŒå…¥å‘é‡
def message_to_distilbert_embedding(messages):
    """
    å°†æ¶ˆæ¯è½¬åŒ–ä¸º DistilBERT åµŒå…¥å‘é‡
    Args:
        messages (list[str]): æ¶ˆæ¯æ–‡æœ¬åˆ—è¡¨
    Returns:
        torch.Tensor: æ¶ˆæ¯çš„å¹³å‡åµŒå…¥å‘é‡
    """
    # ä½¿ç”¨ GPU å¤„ç† Tokenizer è¾“å…¥
    inputs = tokenizer(messages, padding=True, truncation=True, return_tensors="pt").to(device)
    # ä½¿ç”¨ GPU è¿è¡Œæ¨¡å‹
    outputs = model(**inputs)
    # å°†éšè—çŠ¶æ€çš„æœ€åä¸€å±‚å¹³å‡æ± åŒ–
    return outputs.last_hidden_state.mean(dim=1).cpu().detach().numpy()  # è¿”å›åˆ° CPU è¿›è¡Œå­˜å‚¨

# æ‰¹é‡å¤„ç†æ¶ˆæ¯åµŒå…¥
batch_size = 32
message_embeddings = []
for i in range(0, len(processed_df), batch_size):
    batch_messages = processed_df['message'].iloc[i:i+batch_size].tolist()
    batch_embeddings = message_to_distilbert_embedding(batch_messages)
    message_embeddings.extend(batch_embeddings)

# æ·»åŠ åˆ° DataFrame
processed_df['message_embedding'] = message_embeddings
import numpy as np
processed_df['log_time_diff'] = np.log1p(processed_df['time_diff'])

import torch.nn as nn

user_embedding_layer = nn.Embedding(num_embeddings=len(user_mapping), embedding_dim=100)
processed_df['user_embedding'] = processed_df['user_id'].apply(
    lambda x: user_embedding_layer(torch.tensor(x))
)

def prepare_tcn_input(data):
    embeddings = torch.stack([torch.tensor(e) for e in data['message_embedding'].values.tolist()])
    time_features = torch.tensor(data['log_time_diff'].values).unsqueeze(1)
    user_features = torch.stack([torch.tensor(u) for u in data['user_embedding'].values.tolist()])
    return torch.cat([embeddings, time_features, user_features], dim=1)

tcn_inputs = prepare_tcn_input(processed_df)
tcn_inputs = prepare_tcn_input(processed_df).float()
print(tcn_inputs.shape)  # åº”è¾“å‡º [batch_size, feature_dim]

import torch
import torch.nn as nn
import torch.nn.functional as F

# å±€éƒ¨æ³¨æ„åŠ›å±‚ï¼Œç”¨äºè®¡ç®—æ¶ˆæ¯ä¸Šä¸‹æ–‡ä¾èµ–æ€§
class LocalAttention(nn.Module):
    def __init__(self, hidden_size, num_neighbors=5):
        super(LocalAttention, self).__init__()
        self.num_neighbors = num_neighbors
        self.W_q = nn.Linear(hidden_size, hidden_size)
        self.W_k = nn.Linear(hidden_size, hidden_size)
        self.W_v = nn.Linear(hidden_size, hidden_size)

    def forward(self, hidden_states):
        batch_size, seq_length, hidden_size = hidden_states.shape
        outputs = []
        for t in range(seq_length):
            query = self.W_q(hidden_states[:, t, :])
            start_idx = max(0, t - self.num_neighbors)
            context = hidden_states[:, start_idx:t+1, :]
            keys = self.W_k(context)
            scores = torch.bmm(query.unsqueeze(1), keys.permute(0, 2, 1)).squeeze(1)
            attention_weights = F.softmax(scores, dim=1)
            values = self.W_v(context)
            attended_representation = torch.bmm(attention_weights.unsqueeze(1), values).squeeze(1)
            outputs.append(attended_representation)
        return torch.stack(outputs, dim=1)

# TCN + Local Attention æ¨¡å‹ï¼Œè¾“å‡ºä¸€ä¸ªå›ºå®šå‘é‡ï¼ˆä¾‹å¦‚ç”¨äºåç»­ä»»åŠ¡ï¼‰
class TCNWithLocalAttention(nn.Module):
    def __init__(self, input_size, hidden_size, num_channels, kernel_size=3, num_neighbors=5):
        """
        å‚æ•°è¯´æ˜ï¼š
          input_size   : æ‹¼æ¥åç‰¹å¾çš„ç»´åº¦ï¼Œä¾‹å¦‚ embedding_dim + 1 + user_dimï¼ˆä¾‹å¦‚ 300 + 1 + 100 = 401ï¼‰
          hidden_size  : ç»è¿‡å…¨è¿æ¥å±‚åå¾—åˆ°çš„å‘é‡ç»´åº¦ï¼Œä¹Ÿæ˜¯æœ€ç»ˆè¾“å‡ºå‘é‡çš„ç»´åº¦ï¼ˆä¾‹å¦‚ 128ï¼‰
          num_channels : TCN å±‚çš„è¾“å‡ºé€šé“æ•°ï¼ˆä¾‹å¦‚ 256ï¼‰
          kernel_size  : å·ç§¯æ ¸å¤§å°
          num_neighbors: å±€éƒ¨æ³¨æ„åŠ›å±‚ä¸­è€ƒè™‘çš„é‚»å±…æ•°é‡
        """
        super(TCNWithLocalAttention, self).__init__()
        self.tcn = nn.Conv1d(input_size, num_channels, kernel_size, padding=kernel_size//2)
        self.fc = nn.Linear(num_channels, hidden_size)
        self.local_attention = LocalAttention(hidden_size, num_neighbors)

    def forward(self, x):
        """
        å‚æ•°:
          x: è¾“å…¥ tensorï¼Œå¯ä»¥æ˜¯äºŒç»´ [batch_size, input_size]ï¼ˆè‡ªåŠ¨æ‰©å±•ä¸º [batch_size, input_size, 1]ï¼‰
             æˆ–ä¸‰ç»´ [batch_size, input_size, sequence_length]
        è¿”å›:
          vector_output: æ¯ä¸ªæ ·æœ¬çš„å‘é‡è¡¨ç¤ºï¼Œå½¢çŠ¶ä¸º [batch_size, hidden_size]
        """
        # å¦‚æœè¾“å…¥ x æ˜¯äºŒç»´ï¼Œåˆ™åœ¨æœ€åä¸€ç»´æ‰©å±•ä¸º 1ï¼ˆä»£è¡¨å•ä¸€æ—¶é—´æ­¥ï¼‰
        if x.dim() == 2:
            x = x.unsqueeze(-1)  # å˜ä¸º [batch_size, input_size, 1]
        # ç»è¿‡ TCN å±‚ï¼Œè¾“å‡ºå½¢çŠ¶ä¸º [batch_size, num_channels, sequence_length]
        x = self.tcn(x)
        x = torch.relu(x)
        # è½¬ç½®ä¸º [batch_size, sequence_length, num_channels] ä»¥ä¾¿å…¨è¿æ¥å±‚å¤„ç†
        hidden_states = self.fc(x.permute(0, 2, 1))
        # é€šè¿‡å±€éƒ¨æ³¨æ„åŠ›è·å¾—ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¾“å‡ºå½¢çŠ¶ä¸º [batch_size, sequence_length, hidden_size]
        attended_hidden_states = self.local_attention(hidden_states)
        # è¿™é‡Œé‡‡ç”¨å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä½œä¸ºå‘é‡ï¼ˆä½ ä¹Ÿå¯ä»¥æ”¹ä¸ºå…¨å±€æ± åŒ–ï¼‰
        vector_output = attended_hidden_states[:, -1, :]
        return vector_output

import torch.optim as optim

# ä¸å†ä¾èµ–åŸæ¥çš„ embedding_dim/user_dim å®šä¹‰ï¼Œç›´æ¥ä½¿ç”¨æ•°æ®ä¸­çš„ç‰¹å¾æ•°
input_size = 869
hidden_size = 128     # æœ€ç»ˆè¾“å‡ºå‘é‡çš„ç»´åº¦
num_channels = 256
epochs = 10

# åˆå§‹åŒ– TCN æ¨¡å‹
model = TCNWithLocalAttention(input_size=tcn_inputs.shape[1], hidden_size=128, num_channels=256)

# å¦‚æœä½ çš„ä¸‹æ¸¸ä»»åŠ¡æ˜¯åˆ†ç±»ä»»åŠ¡ï¼ˆä¾‹å¦‚æœ‰10ä¸ªç±»åˆ«ï¼‰ï¼Œä½ å¯èƒ½éœ€è¦ä¸€ä¸ªåˆ†ç±»å±‚
num_classes = 2
classifier = nn.Linear(hidden_size, num_classes)

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=0.001)

# ç¡®ä¿ targets æ˜¯ä¸€ä¸ª LongTensorï¼Œå½¢çŠ¶ä¸º [batch_size]
# ä¾‹å¦‚ï¼štargets = torch.randint(0, num_classes, (tcn_inputs.shape[0],))
# è¯·æ ¹æ®ä½ çš„ä»»åŠ¡å‡†å¤‡åˆé€‚çš„ targets
# è¿™é‡Œä»…ä¸ºç¤ºä¾‹
targets = torch.randint(0, num_classes, (tcn_inputs.shape[0],))

# è®­ç»ƒå¾ªç¯
for epoch in range(epochs):
    optimizer.zero_grad()
    # æ¨¡å‹æ¥æ”¶ tcn_inputsï¼Œå¦‚æœè¾“å…¥ä¸ºäºŒç»´ï¼Œå†…éƒ¨ä¼šè‡ªåŠ¨æ‰©å±•ä¸º [batch_size, input_size, 1]
    vector_output = model(tcn_inputs)  # è¾“å‡ºå½¢çŠ¶ä¸º [batch_size, hidden_size]
    logits = classifier(vector_output)  # å°†å‘é‡æ˜ å°„åˆ°ç±»åˆ«ï¼Œè¾“å‡ºå½¢çŠ¶ä¸º [batch_size, num_classes]
    loss = criterion(logits, targets)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}")

# è®­ç»ƒç»“æŸå
model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼

with torch.no_grad():
    # tcn_inputs ä¸ºä½ çš„é¢„å¤„ç†è¾“å…¥æ•°æ®ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨åˆ¤æ–­è¾“å…¥ç»´åº¦ï¼ˆäºŒç»´æˆ–ä¸‰ç»´ï¼‰
    vector_output = model(tcn_inputs)  # è¾“å‡ºå½¢çŠ¶ä¸º [batch_size, hidden_size]
    print("è¾“å‡ºå‘é‡çš„ shape:", vector_output.shape)
    print("éƒ¨åˆ†å‘é‡å†…å®¹ï¼š", vector_output[:5])  # æ‰“å°å‰5ä¸ªæ ·æœ¬çš„å‘é‡

# å¦‚æœä½ æƒ³ä¿å­˜è¿™äº›å‘é‡åˆ°æ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨ numpy ä¿å­˜ä¸º .npy æ–‡ä»¶
import numpy as np
# å¦‚æœæ¨¡å‹åœ¨ GPU ä¸Šï¼Œè¯·å…ˆå°†å…¶è½¬åˆ° CPU
vector_np = vector_output.cpu().numpy()
np.save("output_vectors.npy", vector_np)
print("å‘é‡å·²ç»ä¿å­˜åˆ° output_vectors.npy")
# æˆ–è€…ä½¿ç”¨ Pandas ä¿å­˜ä¸º CSV æ–‡ä»¶ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥è‡ªåŠ¨å¤„ç†è¡Œå·ç­‰é—®é¢˜
df = pd.DataFrame(vector_np)
df.to_csv("output_vectors_pandas.csv", index=False)
print("å‘é‡å·²ä¿å­˜åˆ° output_vectors_pandas.csv")

import pandas as pd
import numpy as np
import string
import torch
#from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
#å¯å‘å¼ç‰¹å¾è¯»å–
# è¯»å–æ•°æ®
file_path = "processed_chat_data_5.csv"
df = pd.read_csv(file_path)

# ç–‘é—®å…³é”®è¯åˆ†ç±»ï¼ˆæ‰€æœ‰ç±»åˆ«ï¼‰
chinese_keywords_by_character = {
    "å“ª": ["å“ªä¸€å¤©", "å“ªä¸ª", "å“ªä¸ªå›½å®¶", "å“ªä¸ªåŸå¸‚", "å“ªä¸ªåœ°æ–¹", "å“ªä¸ªåŒºåŸŸ",
           "å“ªä¸€ä½", "å“ªä¸€ç±»", "å“ªä¸€ç§", "å“ªä¸€æ¬¾", "å“ªä¸€å®¶", "å“ªé‡Œ", "å“ªå„¿", "å“ªäº›"],
    "ä½•": ["ä½•æ—¶", "ä½•å¤„", "ä½•", "å› ä½•", "å¦‚ä½•", "ä¸ºä½•", "å¦‚ä½•åš",
           "å¦‚ä½•è§£å†³", "æ€ä¹ˆå›äº‹", "ä¸ºä»€ä¹ˆ", "ä¸ºä½•å¦‚æ­¤", "ä¸ºä»€ä¹ˆè¿™æ ·"],
    "æ€": ["æ€ä¹ˆ", "æ€ä¹ˆåŠ", "æ€ä¹ˆåš", "æ€æ ·", "æ€ä¹ˆæ ·", "æ€æ ·åš",
           "æ€ä¹ˆè§£å†³", "æ€ä¹ˆå›äº‹", "æ€ä¹ˆæ ·"],
    "å¤š": ["å¤š", "å¤šä¹ˆ", "å¤šå¤§", "å¤šé«˜", "å¤šé•¿", "å¤šå®½", "å¤šæ·±",
           "å¤šé‡", "å¤šå¿«", "å¤šæ…¢", "å¤šè´µ", "å¤šä¾¿å®œ", "å¤šéš¾", "å¤šå®¹æ˜“",
           "å¤šå°‘", "å¤šä¹…", "å¤šé•¿æ—¶é—´"],
    "ä»€": ["ä»€ä¹ˆ", "ä»€ä¹ˆæ—¶å€™", "ä»€ä¹ˆæ„æ€", "ä»€ä¹ˆæƒ…å†µ",
           "ä»€ä¹ˆé—®é¢˜", "ä»€ä¹ˆåŸå› ", "ä»€ä¹ˆç»“æœ", "å¹²ä»€ä¹ˆ"],
    "å‡ ": ["å‡ ", "å‡ æ—¶", "å‡ ä¸ª", "å‡ ä»¶", "å‡ æ¬¡", "å‡ å¤©",
           "å‡ å¹´", "å‡ ä¸ªæœˆ", "å‡ å°æ—¶", "å‡ åˆ†é’Ÿ"],
    "æ˜¯": ["æ˜¯å¦", "æ˜¯ä¸æ˜¯"],
    "èƒ½": ["èƒ½å¦", "èƒ½ä¸èƒ½"],
    "å¯": ["å¯å¦", "å¯ä¸å¯ä»¥"],
    "è°": ["è°", "å“ªä½", "ä»€ä¹ˆäºº"],
    "è¡Œ": ["è¡Œä¸è¡Œ"],
    "å¥½": ["å¥½ä¸å¥½"],
    "å¯¹": ["å¯¹ä¸å¯¹"]
}

# è®¡ç®— keyword_overlap
def calculate_keyword_overlap(messages, keyword_dict):
    keyword_overlap_scores = []
    contains_question_mark = []

    for msg in messages:
        total_count = sum(1 for key, words in keyword_dict.items() for word in words if word in msg)

        # è®¡ç®— keyword_overlapï¼ˆé˜²æ­¢é™¤é›¶ï¼‰
        score = total_count / len(msg) if len(msg) > 0 else 0
        keyword_overlap_scores.append(score)

        # è®¡ç®— contains_question_mark
        contains_question_mark.append(1 if "ï¼Ÿ" in msg else 0)

    return keyword_overlap_scores, contains_question_mark

# è®¡ç®— keyword_overlap å’Œ contains_question_mark
df['keyword_overlap'], df['contains_question_mark'] = calculate_keyword_overlap(df['message'], chinese_keywords_by_character)

# è®¡ç®—æ¶ˆæ¯é•¿åº¦
df['message_length'] = df['message'].apply(len)

# è®¡ç®—æ ‡ç‚¹ç¬¦å·æ¯”ä¾‹
df['punctuation_ratio'] = df['message'].apply(lambda x: sum(1 for c in x if c in string.punctuation) / len(x) if len(x) > 0 else 0)

# è®¡ç®—æ—¶é—´é—´éš”å½’ä¸€åŒ–
df['log_time_diff'] = np.log1p(df['time_diff'])  # log(1 + time_diff)

# æ˜¯å¦é•¿æ—¶é—´æœªå›å¤ï¼ˆå¦‚è¶…è¿‡ 5 åˆ†é’Ÿï¼‰
df['long_silence'] = (df['time_diff'] > 300).astype(int)  # 1 è¡¨ç¤ºé•¿æ—¶é—´æœªå›å¤

from snownlp import SnowNLP

# ä½¿ç”¨ SnowNLP è®¡ç®—æƒ…æ„Ÿå¾—åˆ†
def sentiment_score_snownlp(messages):
    return [SnowNLP(msg).sentiments for msg in messages]

# æ›´æ–°æƒ…æ„Ÿå¾—åˆ†
df['sentiment_score'] = sentiment_score_snownlp(df['message'])

# è®¡ç®—æƒ…æ„Ÿå˜åŒ–
df['sentiment_shift'] = abs(df['sentiment_score'].diff().fillna(0))

from sklearn.metrics.pairwise import cosine_similarity

# æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ çš„æ¶ˆæ¯åµŒå…¥å­˜å‚¨åœ¨ processed_df ä¸­
# å¦‚æœæ¶ˆæ¯åµŒå…¥å®é™…ä¸Šå­˜æ”¾åœ¨ df ä¸­ï¼Œè¯·æ”¹ä¸º df['message_embedding']
embeddings = torch.stack([torch.tensor(e, dtype=torch.float32) for e in processed_df['message_embedding'].tolist()])
embeddings = embeddings.numpy()  # è½¬æ¢ä¸º numpy æ•°ç»„

# è®¡ç®—å¥å­åµŒå…¥çš„ä½™å¼¦ç›¸ä¼¼åº¦
similarities = cosine_similarity(embeddings)
df['topic_deviation'] = [0] + [1 - similarities[i, i - 1] for i in range(1, len(similarities))]

# é€‰æ‹©å¯å‘å¼ç‰¹å¾
heuristic_features = df[['keyword_overlap', 'contains_question_mark', 'message_length','punctuation_ratio',
                          'long_silence', 'sentiment_shift', 'topic_deviation']]

# è½¬æ¢ä¸º PyTorch å¼ é‡
heuristic_vector = torch.tensor(heuristic_features.values, dtype=torch.float32)

# ä¿å­˜å¯å‘å¼ç‰¹å¾ä¸º .npy æ–‡ä»¶
np.save("heuristic_features.npy", heuristic_vector.numpy())

print("å¯å‘å¼ç‰¹å¾æ•°æ®å·²ä¿å­˜ä¸º heuristic_features.npy")


# ä¿å­˜ä¸º CSV
heuristic_features.to_csv("heuristic_features.csv", index=False, encoding="utf-8")
print("å¯å‘å¼ç‰¹å¾æ•°æ®å·²ä¿å­˜ä¸º heuristic_features.csv")

import numpy as np
from sklearn.preprocessing import StandardScaler

# åŠ è½½ .npy æ–‡ä»¶
heuristic_features = np.load("heuristic_features.npy")  # å½¢çŠ¶: (n_samples, heuristic_dim)
output_vectors = np.load("output_vectors.npy")          # å½¢çŠ¶: (n_samples, 128) å‡è®¾è¿™æ˜¯è¯­ä¹‰å‘é‡

# ä½¿ç”¨ StandardScaler å¯¹å¯å‘å¼ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
scaler_heuristic = StandardScaler()
heuristic_features_standardized = scaler_heuristic.fit_transform(heuristic_features)

# ä½¿ç”¨ StandardScaler å¯¹è¯­ä¹‰å‘é‡è¿›è¡Œæ ‡å‡†åŒ–
scaler_output = StandardScaler()
output_vectors_standardized = scaler_output.fit_transform(output_vectors)

# ç¤ºä¾‹ï¼šå°†ä¸¤ä¸ªæ ‡å‡†åŒ–åçš„ç‰¹å¾æŒ‰åˆ—æ‹¼æ¥
combined_features = np.hstack((output_vectors_standardized, heuristic_features_standardized))
print("Combined Features Shape:", combined_features.shape)

# ä¿å­˜æ‹¼æ¥åçš„ç‰¹å¾
np.save("combined_features_standardized.npy", combined_features)
print("æ•´åˆåçš„æ ‡å‡†åŒ–ç‰¹å¾å·²ä¿å­˜ä¸º combined_features_standardized.npy")

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# åŠ è½½æ•´åˆåçš„æ ‡å‡†åŒ–ç‰¹å¾
print("å‘é‡å·²ç»ä¿å­˜åˆ° combined_features_standardized.npy")
combined_features = np.load("combined_features_standardized.npy")

# æ£€æŸ¥æ•°æ®å½¢çŠ¶
print("Combined Features Shape:", combined_features.shape)

# ä½¿ç”¨ KMeans èšç±»
n_clusters = 2  # å‡è®¾åˆ†ä¸ºä¸¤ç±»ï¼šè¯é¢˜è½¬ç§»å’Œéè¯é¢˜è½¬ç§»
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
clusters = kmeans.fit_predict(combined_features)

# è¯„ä¼°èšç±»æ•ˆæœ
silhouette_avg = silhouette_score(combined_features, clusters)
print(f"Silhouette Score for KMeans: {silhouette_avg:.4f}")

# è®¡ç®—èšç±»ç»“æœæ¯”ä¾‹
cluster_counts = np.bincount(clusters)
total_samples = len(clusters)
cluster_ratios = cluster_counts / total_samples

# æ˜¾ç¤ºå„ç±»çš„æ¯”ä¾‹
for i, ratio in enumerate(cluster_ratios):
    print(f"Cluster {i} Ratio: {ratio:.4%}")

# ä½¿ç”¨ PCA é™ç»´åˆ° 2D ä»¥ä¾¿å¯è§†åŒ–
pca = PCA(n_components=2)
combined_features_reduced = pca.fit_transform(combined_features)

plt.figure(figsize=(8, 6))
plt.scatter(combined_features_reduced[:, 0], combined_features_reduced[:, 1], c=clusters, cmap='viridis', s=5)
plt.title("Topic Shift Clusters (KMeans on Combined Standardized Features)")
plt.colorbar(label="Cluster")
plt.xlabel("PCA Dimension 1")
plt.ylabel("PCA Dimension 2")
plt.show()

# åŠ è½½åŸå§‹æ•°æ®
original_data = pd.read_csv("processed_chat_data_5.csv")
original_data['cluster_label'] = clusters

# ä¿å­˜èšç±»ç»“æœåˆ°æ–‡ä»¶
output_csv_path = "topic_shift_clusters_kmeans.csv"
original_data.to_csv(output_csv_path, index=False, encoding="utf-8")
print(f"èšç±»ç»“æœå·²ä¿å­˜ä¸º {output_csv_path}")

import csv

input_file = 'topic_shift_clusters_kmeans.csv'      # è¾“å…¥æ–‡ä»¶
output_file = 'new_topic_shift_clusters_kmeans.csv'  # è¾“å‡ºæ–°æ–‡ä»¶

with open(input_file, 'r', newline='', encoding='utf-8') as infile:
    reader = csv.reader(infile)
    rows = list(reader)

# å‡è®¾æ–‡ä»¶ç¬¬ä¸€è¡Œä¸ºæ ‡é¢˜ï¼Œä¸” cluster_label åœ¨æœ€åä¸€åˆ—
if rows:
    header = rows[0]
    # å¦‚æœéœ€è¦ç¡®ä¿æ“ä½œçš„æ˜¯ cluster_label åˆ—ï¼Œä¹Ÿå¯æ£€æŸ¥ header[-1]
    if header[-1] != 'cluster_label':
        print("è­¦å‘Šï¼šæœ€åä¸€åˆ—çš„åç§°ä¸æ˜¯ 'cluster_label'")
    rows[0] = header

    # éå†æ•°æ®è¡Œï¼Œäº¤æ¢ 0 ä¸ 1
    for i in range(1, len(rows)):
        # è·å–æœ€åä¸€åˆ—çš„å€¼
        val = rows[i][-1].strip()
        # å¦‚æœå€¼ä¸º '1'ï¼Œåˆ™ä¿®æ”¹ä¸º '0'ï¼Œåä¹‹å¦‚æœä¸º '0'ï¼Œåˆ™ä¿®æ”¹ä¸º '1'
        if val == '1':
            rows[i][-1] = '0'
        elif val == '0':
            rows[i][-1] = '1'
        # å…¶ä»–å€¼ä¿æŒä¸å˜

# å†™å…¥æ–°çš„ CSV æ–‡ä»¶
with open(output_file, 'w', newline='', encoding='utf-8') as outfile:
    writer = csv.writer(outfile)
    writer.writerows(rows)

print(f"å¤„ç†å®Œæˆï¼Œæ–°çš„æ–‡ä»¶å·²è¾“å‡ºä¸º {output_file}")

import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.cluster import KMeans
from gensim.models import Word2Vec
import re
import string
import torch
from transformers import AutoTokenizer, AutoModel
import stopwordsiso as stopwords

# åŠ è½½æ•°æ®
data_path = "processed_chat_data_5.csv"
df = pd.read_csv(data_path)

texts = df['message'].astype(str).tolist()

# åŠ è½½ä¸­æ–‡åœç”¨è¯
stopwords_zh = stopwords.stopwords('zh')

# SnowNLPåˆ†è¯å®‰å…¨ç‰ˆï¼ˆä¸¥æ ¼è¿‡æ»¤ç©ºä¸²æˆ–å¼‚å¸¸ä¸²ï¼‰
def preprocess_snownlp(text):
    if not isinstance(text, str):
        return ""
    text = re.sub(r"\s+", "", text)  # å»é™¤æ‰€æœ‰ç©ºç™½ç¬¦
    text = re.sub(f"[{string.punctuation}]", "", text)
    if len(text.strip()) < 1:
        return ""
    try:
        words = SnowNLP(text).words
        words = [w for w in words if w.strip() and w not in stopwords_zh and len(w.strip()) > 1]
        return ' '.join(words)
    except:
        return ""

# å¤„ç†æ–‡æœ¬ï¼Œå¹¶å»é™¤åˆ†è¯åä¸ºç©ºçš„æ–‡æœ¬
processed_texts = [preprocess_snownlp(text) for text in texts]

# åªä¿ç•™æœ‰æ•ˆåˆ†è¯ç»“æœ
valid_indices = [i for i, text in enumerate(processed_texts) if text.strip()]
processed_texts = [processed_texts[i] for i in valid_indices]
df_valid = df.iloc[valid_indices].reset_index(drop=True)

# --- æ–¹æ³•ä¸€ï¼šTF-IDFèšç±» ---
tfidf_vectorizer = TfidfVectorizer(max_features=50)
tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)
labels_tfidf = KMeans(n_clusters=2, random_state=42, n_init=10).fit_predict(tfidf_matrix)
df_valid['tfidf_cluster'] = labels_tfidf

# --- æ–¹æ³•äºŒï¼šLDAèšç±» ---
lda_vectorizer = TfidfVectorizer(max_features=50)
lda_tfidf_matrix = lda_vectorizer.fit_transform(processed_texts)
lda = LatentDirichletAllocation(n_components=2, random_state=42)
lda_matrix = lda.fit_transform(lda_tfidf_matrix)
labels_lda = KMeans(n_clusters=2, random_state=42, n_init=10).fit_predict(lda_matrix)
df_valid['lda_cluster'] = labels_lda

# --- æ–¹æ³•ä¸‰ï¼šWord2Vec ç›‘ç£å­¦ä¹  ---
# ä½¿ç”¨Word2Vecç”Ÿæˆæ–‡æœ¬ç‰¹å¾
tokenized_texts = [text.split() for text in processed_texts]
w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=2, workers=4)

# è·å–Word2Vecæ–‡æ¡£å‘é‡ï¼ˆé€šè¿‡å–æ¯ä¸ªè¯çš„è¯å‘é‡çš„å¹³å‡å€¼ï¼‰
def get_w2v_vector(text):
    vectors = [w2v_model.wv[word] for word in text if word in w2v_model.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v_model.vector_size)

w2v_vectors = np.array([get_w2v_vector(text) for text in tokenized_texts])

# ä½¿ç”¨SVMè¿›è¡Œç›‘ç£å­¦ä¹ 
clf = SVC()  # ä½ å¯ä»¥é€‰æ‹©å…¶ä»–åˆ†ç±»å™¨ï¼Œå¦‚é€»è¾‘å›å½’ã€éšæœºæ£®æ—ç­‰
clf.fit(w2v_vectors, df_valid['label'])  # ä½¿ç”¨æ ‡ç­¾æ•°æ®è¿›è¡Œè®­ç»ƒ

# é¢„æµ‹å¹¶è¯„ä¼°
predictions_w2v = clf.predict(w2v_vectors)
accuracy_w2v = accuracy_score(df_valid['label'], predictions_w2v)
df_valid['w2v_pred'] = predictions_w2v
print(f'Word2Vec Accuracy: {accuracy_w2v}')

# --- æ–¹æ³•å››ï¼šDistilBERTèšç±» ---
device = 'cuda' if torch.cuda.is_available() else 'cpu'
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-multilingual-cased')
model = AutoModel.from_pretrained('distilbert-base-multilingual-cased').to(device)

def get_distilbert_embedding(text):
    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
    return embeddings.squeeze()

# ä½¿ç”¨DistilBERTç”Ÿæˆæ–‡æœ¬ç‰¹å¾
distilbert_embeddings = np.array([get_distilbert_embedding(text) for text in df_valid['message'].astype(str).tolist()])

# ä½¿ç”¨KMeansèšç±»ï¼ˆæ— ç›‘ç£ï¼‰
labels_distilbert = KMeans(n_clusters=2, random_state=42, n_init=10).fit_predict(distilbert_embeddings)
df_valid['distilbert_cluster'] = labels_distilbert


# å­˜å‚¨æœ€ç»ˆç»“æœ
df_valid.to_csv('chat_data_all_clusters.csv', index=False)

import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score
import numpy as np

# WindowDiffè®¡ç®—å‡½æ•°
def windowdiff(seg1, seg2, k):
    n = len(seg1)
    wd = 0
    for i in range(n - k):
        wd += abs(sum(seg1[i:i+k]) - sum(seg2[i:i+k])) > 0
    return wd / (n - k)

# åŠ è½½ä¹‹å‰èšç±»ç»“æœçš„æ•°æ®
df_clusters = pd.read_csv('chat_data_all_clusters.csv')
df_clusters = df_clusters.dropna(subset=['label', 'tfidf_cluster', 'lda_cluster', 'w2v_pred', 'distilbert_cluster'])

# åŠ è½½æ–°ä¸Šä¼ çš„æ•°æ®ï¼ˆä½œä¸ºå¯¹æ¯”ï¼‰
df_compare = pd.read_csv('topic_shift_clusters_kmeans.csv')
df_compare = df_compare.dropna(subset=['label', 'cluster_label'])

# ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´ï¼ˆæˆªæ–­è‡³æœ€çŸ­çš„é•¿åº¦ï¼‰
min_len = min(len(df_clusters), len(df_compare))
df_clusters = df_clusters.iloc[:min_len]
df_compare = df_compare.iloc[:min_len]

# çœŸå®æ ‡ç­¾
true_labels = df_clusters['label'].astype(int).values

# å„æ–¹æ³•çš„é¢„æµ‹æ ‡ç­¾
predictions = {
    'TFIDF': df_clusters['tfidf_cluster'].astype(int).values,
    'LDA': df_clusters['lda_cluster'].astype(int).values,
    'Word2Vec': df_clusters['w2v_pred'].astype(int).values,
    'DistilBERT': df_clusters['distilbert_cluster'].astype(int).values,
    'TopicShift_KMeans': df_compare['cluster_label'].astype(int).values
}

# è®¡ç®—çª—å£å¤§å° k
k = int(round(np.sqrt(len(true_labels))/2))

# è®¡ç®—å¹¶å­˜å‚¨æ‰€æœ‰æ–¹æ³•çš„ç»“æœ
results = []

for method, pred_labels in predictions.items():
    accuracy = accuracy_score(true_labels, pred_labels)
    f1 = f1_score(true_labels, pred_labels, average='macro')  # ä½¿ç”¨macroå¹³å‡æ¥å¤„ç†å¤šç±»åˆ«é—®é¢˜
    recall = recall_score(true_labels, pred_labels, average='macro')  # è®¡ç®—å®å¹³å‡çš„å¬å›ç‡
    precision = precision_score(true_labels, pred_labels, average='macro')  # è®¡ç®—å®å¹³å‡çš„ç²¾ç¡®ç‡
    wd_score = windowdiff(true_labels, pred_labels, k)
    results.append({
        'Method': method,
        'Accuracy': accuracy,
        'F1 Score': f1,  # æ·»åŠ F1åˆ†æ•°
        'Recall': recall,  # æ·»åŠ å¬å›ç‡
        'Precision': precision,  # æ·»åŠ ç²¾ç¡®ç‡
        'WindowDiff': wd_score
    })

# å±•ç¤ºå¯¹æ¯”ç»“æœ
results_df = pd.DataFrame(results)
print("ğŸ“Š å¯¹æ¯”å®éªŒç»“æœï¼š")
print(results_df)
# è¾“å‡ºåˆ° txt æ–‡ä»¶
with open("experiment_results_D5.txt", "w") as f:
    f.write("ğŸ“Š å¯¹æ¯”å®éªŒç»“æœï¼š\n")
    f.write(results_df.to_string(index=False))

print("å®éªŒç»“æœå·²ä¿å­˜åˆ° 'experiment_results_D5.txt'")